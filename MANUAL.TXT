INSTRUCTION MANUAL
-------------------------------------------------------------------------------

Written by Anees Ahmed.



===============================================================================
SETUP
===============================================================================


1)  Use a GNU/Linux system (preferebly, Ubuntu).


2)  Make sure Python v2.7 is installed on the system.

    Run "python --version" to check.

    If multiple Python versions are installed, use "python2.7" instead of
    "python" to be specific.


3)  Install PRESTO (https://www.cv.nrao.edu/~sransom/presto/).

    It is recommended to use the system package manager wherever possible to
    install a package which contains the dependency required by PRESTO.

    Make sure to set up all the global variables carefully.

    It is discouraged to parmanently modify the global variables. It is
    recommended that a shell script be created instead which sets these
    variables up temporarily for a shell session. For example, see the Bash
    script "enable_presto_env.bash", it assumes PRESTO and TEMPO were installed
    in "/opt/" and other dependencies (e.g. PGPLOT) were installed in standard
    paths by using the system package manager, specifically APT on Ubuntu.


4)  From here onwards, all commands are written assuming the current working
    directory is the main project folder.

    For example, in Bash, use "cd" command to change the current working
    directory.


5)  Install "virtualenv" (https://virtualenv.pypa.io/en/latest/).

    It is discouraged to use "pip" for installing "virtualenv", as it can break
    the system. It is recommended to use the system package manager to install
    a package which contains "virtualenv".

    On Ubuntu, run "sudo apt install virtualenv"


6)  Create a virtual environment just inside the main project folder.

    Run "virtualenv -p python2.7 virtualenv".


7)  Activate the virtual environment for the current shell session.

    Run "source virtualenv/bin/activate".


8)  Run the shell script which sets the global variables up for the current
    shell session, unless they are already set permanently.

    For example, in Bash, to use the example shell script provided
    "enable_presto_env.bash", run "source enable_presto_env.bash".


9)  Make sure the global varibales are set up correctly.

    Run "show_pfd" to check.


10) Install the required Python packages into the virtual environment,
    using the "requirements.txt" file.

    Run "pip install -r requirements.txt".


11) Make sure the required Python packages are installed correctly.

    To check, run "python" to launch the Python interpreter, and then type in
    "import tensorflow" and press Enter key. Also check more by trying
    "import numpy" and "import pathlib2".



===============================================================================
TRAIN
===============================================================================


1)  Activate the virtual environment for the current shell session.

    Run "source virtualenv/bin/activate".


2)  Run the shell script which sets the global variables up for the current
    shell session, unless they are already set permanently.

    For example, in Bash, to use the example shell script provided
    "enable_presto_env.bash", run "source enable_presto_env.bash".


3)  Generate PS files for every PFD file in the dataset folder.

    The relative heirarchy of the PFD files inside the dataset folder doesn't
    matter. For every "abc.pfd", a "abc.ps" will be created in the same folder.

    Run "python create_ps_from_pfd.py /path/to/data_dir".


4)  Arrange PFD files by moving every PFD file into an either "positive" or
    "negative" named folder. Any PFD file coreesponding to a confirmed pulsar
    must be under a "positive" named folder, while any PFD file coreesponding
    to a confirmed non-pulsar must be under a "negative" named folder.

    One can use the generated PS files to inspect the PFD files. The PS files
    are only for humans segregating the PFD files, and is irrelevant to this
    software. They can even be deleted after this segregation step.

    Again, the relative heirarchy of the PFD files inside the dataset folder
    doesn't matter, except that every PFD file must be inside a folder either
    named "positive" or "negative". There can be multiple "positive" and
    "negative" named folders.


5)  Extract all features from the PFD files and dump them into into a PICKLED
    file.

    A PICKLED file is a container file which stores Python objects inside it.
    This is used so that data structures can be dumped on to disk and be loaded
    back into memory later. This format is not guaranteed to be compatible
    across different machine architectures and operating systems.

    See "https://docs.python.org/2/library/pickle.html" to learn more.

    Run "python create_dataset.py /path/to/data_dir /path/to/dataset.pickled".


6)  Filter out those samples form PICKLED file and store them into a NPZ file,
    which are compatible with the Neural Network.

    A NPZ file is a container file which stores Numpy arrays inside it. This is
    used so that Numpy arrays can be dumped on to disk and be loaded back into
    memory later. This format is guaranteed to be compatible across different
    machine architectures and operating systems.

    See "https://docs.scipy.org/doc/numpy/reference/generated/numpy.lib.format.html"
    to learn more.

    Run
    "python pickled_to_npz.py /path/to/dataset.pickled /path/to/dataset.npz".


7)  Divide the samples in NPZ file into 'test' and 'train' and store them into
    separate NPZ files.

    Samples are automatically stored into "training_data/npy/train.npz" and
    "training_data/npy/test.npz", using a 33% split fraction for 'test'.

    Run "python compile_dataset.py /path/to/dataset.npz".


8)  Start the training the Neural Network.

    This also detects if a previous training session was left off and resumes
    the training from there onwards.

    The trained weigths are stored in a "cnn_tf_model" named folder.

    Run "python train.py", and enter the required number of epochs to train,
    enter "0" to stop training and quit.



===============================================================================
PREDICT
===============================================================================


1)  Activate the virtual environment for the current shell session.

    Run "source virtualenv/bin/activate".


2)  Run the shell script which sets the global variables up for the current
    shell session, unless they are already set permanently.

    For example, in Bash, to use the example shell script provided
    "enable_presto_env.bash", run "source enable_presto_env.bash".


3)  Extract all features from the PFD files and dump them into into a PICKLED
    file. These are fresh new PFD files which have never been seen before by
    the Neural Network.

    The relative heirarchy of the PFD files inside the folder doesn't matter.

    Run "python create_dataset.py /path/to/unseen_data_dir /path/to/unseen_dataset.pickled".


4)  Filter out those samples form PICKLED file and store them into a NPZ file,
    which are compatible with the Neural Network.

    Run "python pickled_to_npz.py /path/to/unseen_dataset.pickled /path/to/unseen_dataset.npz".


5)  Feed the samples to the Neural Network and save the predicion results into
    a CSV file.

    A CSV file is plain-text file which stores rows of tabular data.

    See "https://en.wikipedia.org/wiki/Comma-separated_values" to learn more.

    A lot of popular spreadsheet softwares can also import CSV data.

    Run "python predict.py /path/to/unseen_dataset.npz /path/to/prediction_result.csv".
